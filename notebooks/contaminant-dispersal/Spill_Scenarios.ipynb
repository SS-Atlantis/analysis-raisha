{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salish Sea Oil Spill Scenarios\n",
    "\n",
    "This notebook describes oil particle tracking for spills scenarios in the Salish Sea developed in conjunction with the Canadian Department of Fisheries and Oceans (DFO). The code if for translating particle tracking from *ocean parcels* using the *Salish Sea Cast* grid into input forcing files for *Atlantis* for the *Atlantis* box grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from parcels import AdvectionRK4, VectorField, Variable\n",
    "from parcels import FieldSet, plotTrajectoriesFile, Variable, ScipyParticle, Field\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "sys.path.append('/ocean/rlovindeer/Atlantis/ssam_oceanparcels/Parcels_Utils/particle_tracking/parcels/')\n",
    "from util.seed_particles import get_particles, get_release_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = int(input( ))\n",
    "scenario = {1 : \"5b_Turn_Point_Diluted_bitumen\",\n",
    "            2 : \"6a_VancouverHarbour_BunkerC\",\n",
    "            3 : \"7a_JohnsonStrait_BunkerC\",}\n",
    "print(\"\\nScenario running  :\", scenario[file_id], sep = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating particle movement through Ocean Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernels\n",
    "def WindAdvectionRK4(particle, fieldset, time):\n",
    "    \"\"\"Advection of particles using fourth-order Runge-Kutta integration.\n",
    "     Function needs to be converted to Kernel object before execution\"\"\"\n",
    "    if particle.beached == 0:\n",
    "        wp = fieldset.wind_percentage ## this need to be add to the fieldset\n",
    "        \n",
    "        if wp > 0:\n",
    "\n",
    "            (u1, v1) = fieldset.UV[time, particle.depth, particle.lat, particle.lon]\n",
    "            u1 = u1 * wp\n",
    "            v1 = v1 * wp\n",
    "            lon1, lat1 = (particle.lon + u1*.5*particle.dt, particle.lat + v1*.5*particle.dt)\n",
    "            \n",
    "            (u2, v2) = fieldset.UVwind[time + .5 * particle.dt, particle.depth, lat1, lon1]\n",
    "            u2 = u2 * wp\n",
    "            v2 = v2 * wp\n",
    "            lon2, lat2 = (particle.lon + u2*.5*particle.dt, particle.lat + v2*.5*particle.dt)\n",
    "            \n",
    "            (u3, v3) = fieldset.UVwind[time + .5 * particle.dt, particle.depth, lat2, lon2]\n",
    "            u3 = u3 * wp\n",
    "            v3 = v3 * wp\n",
    "            lon3, lat3 = (particle.lon + u3*particle.dt, particle.lat + v3*particle.dt)\n",
    "            \n",
    "            (u4, v4) = fieldset.UVwind[time + particle.dt, particle.depth, lat3, lon3]\n",
    "            u4 = u4 * wp\n",
    "            v4 = v4 * wp            \n",
    "            \n",
    "            u_wind  = (u1 + 2*u2 + 2*u3 + u4) / 6. * particle.dt\n",
    "            v_wind  = (v1 + 2*v2 + 2*v3 + v4) / 6. * particle.dt\n",
    "            \n",
    "            particle.lon += (u1 + 2*u2 + 2*u3 + u4) / 6. * particle.dt\n",
    "            particle.lat += (v1 + 2*v2 + 2*v3 + v4) / 6. * particle.dt\n",
    "            \n",
    "            particle.beached = 2\n",
    "            \n",
    "def BeachTesting(particle, fieldset, time):\n",
    "    \"\"\" Testing if particles are on land. if 'yes' particle will be removed\"\"\"\n",
    "    if particle.beached == 2:\n",
    "        (u, v) = fieldset.UV[time, particle.depth, particle.lat, particle.lon]\n",
    "        #print(u, v)\n",
    "        if u == 0 and v == 0:\n",
    "            particle.beached = 1\n",
    "        else:\n",
    "            particle.beached = 0\n",
    "\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()\n",
    "\n",
    "def DecayParticle(particle, fieldset, time):\n",
    "    dt = particle.dt\n",
    "    field_decay_value = fieldset.decay\n",
    "    decay = math.exp(-1.0 * dt/field_decay_value)\n",
    "    particle.decay_value = particle.decay_value * decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths\n",
    "currents = Path('/ocean/rlovindeer/Atlantis/Physics/Raw_Transport_Data/')\n",
    "winds = Path('/ocean/rlovindeer/Atlantis/Physics/Wind/')\n",
    "sea_grid = Path('/ocean/rlovindeer/Atlantis/Physics/Grids/ubcSSnBathymetryV17-02_a29d_efc9_4047.nc')\n",
    "air_grid = Path('/ocean/rlovindeer/Atlantis/Physics/Grids/ubcSSaAtmosphereGridV1_0f03_6268_df4b.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salish Sea NEMO Model Grid, Geo-location and Bathymetry, v17-02\n",
    "\n",
    "# Currents\n",
    "u_current = sorted([p for p in currents.glob('2018-01*URaw_variables.nc')])\n",
    "v_current = sorted([p for p in currents.glob('2018-01*VRaw_variables.nc')])\n",
    "\n",
    "filenames = {\n",
    "    'U': {'lon': sea_grid,'lat': sea_grid,'data': u_current},\n",
    "    'V': {'lon': sea_grid,'lat': sea_grid,'data': v_current}\n",
    "            }\n",
    "\n",
    "variables = {'U': 'uVelocity','V': 'vVelocity'}\n",
    "dimensions = {'lon': 'longitude', 'lat': 'latitude', 'time': 'time'}\n",
    "print('creating from_nemo')\n",
    "fieldset = FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "print('creating from_nemo done')\n",
    "\n",
    "fieldset.add_constant('decay', 1.0 * 3600.0)\n",
    "print('add_constant decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRDPS, Salish Sea, Atmospheric Forcing Grid, Geo-location, v1\"\n",
    "\n",
    "wind_paths = sorted([p for p in winds.glob('*_Wind_variables.nc')])\n",
    "wind_filenames = {'lon': os.fspath(air_grid),'lat': os.fspath(air_grid),'data': wind_paths}\n",
    "wind_dimensions = {'lon': 'longitude', 'lat': 'latitude', 'time': 'time'}\n",
    "\n",
    "pprint(wind_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uwind_field = Field.from_netcdf(wind_filenames, ('U_wind', 'u_wind'),\n",
    "                                     wind_dimensions,\n",
    "                                     fieldtype='U',\n",
    "                                     allow_time_extrapolation=True,\n",
    "                                     transpose=False,\n",
    "                                     deferred_load=False)\n",
    "Vwind_field = Field.from_netcdf(wind_filenames, ('V_wind', 'v_wind'),\n",
    "                                     wind_dimensions,\n",
    "                                     fieldtype='V',\n",
    "                                     allow_time_extrapolation=True,\n",
    "                                     transpose=False,\n",
    "                                     deferred_load=False)\n",
    "\n",
    "print('wind data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change longitude for the wind field\n",
    "Uwind_field.grid.lon = Uwind_field.grid.lon - 360\n",
    "Vwind_field.grid.lon = Vwind_field.grid.lon - 360\n",
    "\n",
    "[x_min, x_max, y_min, y_max] = Uwind_field.grid.lonlat_minmax\n",
    "\n",
    "Uwind_field.grid.lonlat_minmax = [x_min - 360, x_max - 360, y_min, y_max]\n",
    "Vwind_field.grid.lonlat_minmax = [x_min - 360, x_max - 360, y_min, y_max]\n",
    "\n",
    "## adding the wind field to the fieldset object\n",
    "fieldset.add_field(Uwind_field)\n",
    "fieldset.add_field(Vwind_field)\n",
    "wind_field = VectorField('UVwind', Uwind_field,  Vwind_field)\n",
    "fieldset.add_vector_field(wind_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind_percentage\n",
    "# We need to do a sensitivity analysis of the percetage of wind to be used here\n",
    "wind_percentage = 1\n",
    "fieldset.add_constant('wind_percentage', wind_percentage/100.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Just in case we want to add a maximum age\n",
    "# fieldset_sum.add_constant('max_age', dispersal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParticle(ScipyParticle):\n",
    "    initial_time = -100\n",
    "    decay_value = Variable('decay_value', dtype=np.float32, initial=1.0)\n",
    "    beached = Variable('beached', dtype=np.int32, initial=0.)\n",
    "    age = Variable('age', dtype=np.int32, initial=0.)\n",
    "\n",
    "# Particle Features\n",
    "num_particles_per_day = 100\n",
    "feature_release_index = 0\n",
    "input_shapefile_name = \"/ocean/rlovindeer/Atlantis/ssam_oceanparcels/SalishSea/Shape_Scenarios/\" + scenario[file_id] + \".shp\"\n",
    "release_depth = -0.1\n",
    "release_start_time = '2018-01-01'  ## winter start on December, Summer Jul - Aug  ## ask Susan about when to do simulation\n",
    "release_end_time = '2018-01-02'\n",
    "release_start_time = np.datetime64(release_start_time)\n",
    "release_end_time = np.datetime64(release_end_time)\n",
    "time_origin = fieldset.U.grid.time_origin.time_origin\n",
    "\n",
    "print('setting up particles')\n",
    "\n",
    "[release_times, p, num_particles] = get_release_times(time_origin, num_particles_per_day, release_start_time, release_end_time)\n",
    "pset = get_particles(fieldset, num_particles, input_shapefile_name, MyParticle, feature_release_index, release_times, release_depth)\n",
    "\n",
    "print(pset)\n",
    "\n",
    "# Building the kernels\n",
    "decay_kernel = pset.Kernel(DecayParticle)\n",
    "beaching_kernel = pset.Kernel(BeachTesting)\n",
    "ForcingWind_kernel = pset.Kernel(WindAdvectionRK4)\n",
    "\n",
    "# Adding to the main kernel\n",
    "my_kernel = AdvectionRK4 + decay_kernel + ForcingWind_kernel + beaching_kernel\n",
    "\n",
    "OP_output_file_name = scenario[file_id] + '_oil_disperse.nc'\n",
    "print(OP_output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.system('rm ' + OP_output_file_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('executing particle kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output properties\n",
    "output_file = pset.ParticleFile(name= OP_output_file_name, outputdt = timedelta(minutes = 60))\n",
    "pset.execute(my_kernel,                 # the kernel (which defines how particles move)\n",
    "             runtime=timedelta(hours = 24*6),   # total length of the run\n",
    "             dt = timedelta(minutes = 60),      # timestep of the kernel\n",
    "             output_file = output_file)         # file name and the time step of the outputs\n",
    "output_file.close()\n",
    "\n",
    "plotTrajectoriesFile(OP_output_file_name);\n",
    "\n",
    "print('particle trajectories completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcing Ocean Parcels output into Atlantis input files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_name = \"/ocean/rlovindeer/Atlantis/ssam_oceanparcels/SalishSea/SalishSea_July172019_2/SalishSea_July172019.shp\"\n",
    "data_df = gpd.read_file(shapefile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLayers = 5;\n",
    "numSites = data_df.shape[0]\n",
    "numTargetSites = numSites\n",
    "\n",
    "outputDT = 60*60 #1 hour\n",
    "\n",
    "stepsPerDay = int(86400.0/ outputDT);\n",
    "numStepsPerDT = stepsPerDay;\n",
    "numStepsPerDT = int(outputDT/3600.0)\n",
    "\n",
    "debug = False\n",
    "\n",
    "inputFileName = OP_output_file_name\n",
    "pfile = xr.open_dataset(str(inputFileName), decode_cf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = np.ma.filled(pfile.variables['lon'], np.nan)\n",
    "lat = np.ma.filled(pfile.variables['lat'], np.nan)\n",
    "time = np.ma.filled(pfile.variables['time'], np.nan)\n",
    "z = np.ma.filled(pfile.variables['z'], np.nan)\n",
    "probs = np.ma.filled(pfile.variables['decay_value'], np.nan)\n",
    "\n",
    "numParticles = lon.shape[0]\n",
    "\n",
    "trackDates = [];\n",
    "\n",
    "for i in range(0,numParticles):\n",
    "    print(time[i][0])\n",
    "    trackDates.append(time[i][0])\n",
    "\n",
    "RDiff = max(trackDates) - min(trackDates)\n",
    "\n",
    "minDate = np.datetime64(\"2018-01-01T00:30:00\");\n",
    "ts = pd.to_datetime(str(minDate))\n",
    "d = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numReleaseDays = 1;\n",
    "numReleaseSteps = numReleaseDays * stepsPerDay;\n",
    "trackLength = len(lon[0]);\n",
    "\n",
    "print('trackLength = ' + str(trackLength))\n",
    "print('numStepsPerDT = ' + str(numStepsPerDT))\n",
    "\n",
    "numSteps = int(trackLength / numStepsPerDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the netcdf output file for Atlantis\n",
    "\n",
    "netcdfFileName = \"Atlantis_\" + scenario[file_id] + \"_dt_\" + str(outputDT) + \".nc\"\n",
    "\n",
    "try:\n",
    "    os.remove(netcdfFileName)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ncfile = Dataset(netcdfFileName, \"w\", format=\"NETCDF4\", clobber=True)\n",
    "\n",
    "# Dimensions\n",
    "time = ncfile.createDimension(\"t\", None)\n",
    "b = ncfile.createDimension(\"b\", numTargetSites)\n",
    "z = ncfile.createDimension(\"z\", numLayers)\n",
    "\n",
    "# Variables\n",
    "times = ncfile.createVariable(\"time\",\"f4\",(\"t\",))\n",
    "oil = ncfile.createVariable(\"oil\",\"f4\",(\"t\", \"b\",))\n",
    "Naphthalene = ncfile.createVariable(\"Naphthalene\",\"f4\",(\"t\", \"b\",))\n",
    "Phenanthrene = ncfile.createVariable(\"Phenanthrene\",\"f4\",(\"t\", \"b\",))\n",
    "Pyrene = ncfile.createVariable(\"Pyrene\",\"f4\",(\"t\", \"b\",))\n",
    "Benzo = ncfile.createVariable(\"Benzo\",\"f4\",(\"t\", \"b\",))\n",
    "\n",
    "# Attributes\n",
    "times.units = \"seconds since \" + d\n",
    "times.dt = str(outputDT);\n",
    "oil.units = \"mgPAH/m^3\"\n",
    "oil.long_name = \"Amount of PAH\"\n",
    "\n",
    "Naphthalene.units = \"mgPAH/m^3\"\n",
    "Naphthalene.long_name = \"Amount of Naphthalene\"\n",
    "\n",
    "Phenanthrene.units = \"mgPAH/m^3\"\n",
    "Phenanthrene.long_name = \"Amount of Phenanthrene\"\n",
    "\n",
    "Pyrene.units = \"mgPAH/m^3\"\n",
    "Pyrene.long_name = \"Amount of Pyrene\"\n",
    "\n",
    "Benzo.units = \"mgPAH/m^3\"\n",
    "Benzo.long_name = \"Amount of Benzo(a)pyrene\"\n",
    "\n",
    "# Populate variables with data\n",
    "timeData = np.arange(0,(numSteps + numReleaseSteps)*outputDT,outputDT)\n",
    "times[:] = timeData;\n",
    "\n",
    "boxDispersal = np.zeros((numSteps + numReleaseSteps, numTargetSites));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partIndex in range(0, numParticles):\n",
    "\n",
    "    trackDateDiff = trackDates[partIndex] - minDate;\n",
    "    trackDateDiff = trackDateDiff/ np.timedelta64(1, 's')\n",
    "\n",
    "    timeOffset = int(abs((trackDateDiff /outputDT)));\n",
    "\n",
    "    for stepIndex in range(0, numSteps):\n",
    "        timeValue = stepIndex + timeOffset\n",
    "\n",
    "        partLon = lon[partIndex][stepIndex * numStepsPerDT];\n",
    "        partLat = lat[partIndex][stepIndex * numStepsPerDT];\n",
    "        partProb = probs[partIndex][stepIndex * numStepsPerDT];\n",
    "\n",
    "        matchFound = 0;\n",
    "\n",
    "        for targetIndex in range(0, numTargetSites):\n",
    "\n",
    "            path = data_df.iloc[targetIndex].geometry\n",
    "            checks = path.contains(Point(partLon, partLat));\n",
    "\n",
    "            if checks:\n",
    "                boxDispersal[timeValue][targetIndex] = boxDispersal[timeValue][targetIndex] + partProb;\n",
    "\n",
    "                # uncomment line below to ignore particle decay during debugging.\n",
    "                #boxDispersal[timeValue][targetIndex] = boxDispersal[timeValue][targetIndex] + 1.0\n",
    "\n",
    "                matchFound = 1\n",
    "                if debug:\n",
    "                    print('At time ' + str(timeValue) + ' Particle (' + str(partIndex) + ') in box ' + str(data_df.iloc[targetIndex].BOX_ID))\n",
    "\n",
    "\n",
    "                break;\n",
    "\n",
    "        if matchFound == 0:\n",
    "            if debug:\n",
    "                print('No match for particle')\n",
    "                print(partLon, partLat)\n",
    "\n",
    "\n",
    "        #break\n",
    "\n",
    "oil[:, :] = boxDispersal;\n",
    "Naphthalene[:, :] = boxDispersal;\n",
    "Phenanthrene[:, :] = boxDispersal;\n",
    "Pyrene[:, :] = boxDispersal;\n",
    "Benzo[:, :] = boxDispersal;\n",
    "\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animating Oil Dispersal Scenario in the Salish Sea Atlantis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = data_df['BOTZ']\n",
    "land_boxes = boxes==0\n",
    "land_boxes = data_df.index[land_boxes]\n",
    "\n",
    "numReleaseDays = RDiff\n",
    "print('numReleaseDays = ' + str(numReleaseDays))\n",
    "\n",
    "numReleaseDTS = int(abs(numReleaseDays/np.timedelta64(1, 'h')));\n",
    "totalNumOfTS = int(numReleaseDTS + trackLength);\n",
    "print('totalNumOfTS = ' + str(totalNumOfTS))\n",
    "print('trackLength = ' + str(trackLength))\n",
    "print(numParticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackLonsPadded = np.zeros((int(numParticles), totalNumOfTS));\n",
    "trackLatsPadded = np.zeros((int(numParticles), totalNumOfTS));\n",
    "particlesAge = np.zeros((int(numParticles), totalNumOfTS));\n",
    "\n",
    "for trackIndex in range(0,numParticles):\n",
    "\n",
    "    print(trackDates[trackIndex])\n",
    "    print(minDate)\n",
    "    trackDateDiff = trackDates[trackIndex] - minDate\n",
    "\n",
    "    print(trackDateDiff/np.timedelta64(1, 'h'))\n",
    "\n",
    "    trackNumsToPad = int(trackDateDiff/np.timedelta64(1, 'h'))\n",
    "    print(trackNumsToPad)\n",
    "\n",
    "    trackLonsPadded[trackIndex][0:trackNumsToPad] = 0;\n",
    "    trackLatsPadded[trackIndex][0:trackNumsToPad] = 0;\n",
    "\n",
    "    trackLonsPadded[trackIndex][trackNumsToPad:trackNumsToPad + trackLength] = lon[:][trackIndex];\n",
    "    trackLatsPadded[trackIndex][trackNumsToPad:trackNumsToPad + trackLength] = lat[:][trackIndex];\n",
    "\n",
    "numSteps = int(trackLength / numStepsPerDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile_prefix = 'boxes'\n",
    "\n",
    "pfile = xr.open_dataset(str(netcdfFileName), decode_cf=True)\n",
    "\n",
    "print(pfile)\n",
    "\n",
    "time = np.ma.filled(pfile.variables['time'], np.nan)\n",
    "\n",
    "oil = np.ma.filled(pfile.variables['Benzo'], np.nan)\n",
    "num_steps = time.shape[0]\n",
    "print(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_max = 80\n",
    "_cmap = cm.viridis\n",
    "file_names = []\n",
    "\n",
    "land_df = data_df.loc[land_boxes]\n",
    "\n",
    "for time_index in range(0, num_steps):\n",
    "\n",
    "    plon = trackLonsPadded[:, time_index]\n",
    "    plat = trackLatsPadded[:, time_index]\n",
    "\n",
    "    plon = plon[plon<0]\n",
    "    plat = plat[plat>0]\n",
    "    time_oil = oil[time_index]\n",
    "\n",
    "    data_df['oil'] = time_oil\n",
    "    data_df.loc[land_boxes, 'oil'] = 10000\n",
    "\n",
    "    ax = data_df.plot(figsize=(15, 15), column = 'oil', cmap = _cmap, vmin=0, vmax=v_max, legend=True)\n",
    "    land_df.plot(ax=ax, color='gray')\n",
    "    ax.scatter(plon, plat, s=10, color='red', zorder=20)\n",
    "\n",
    "    ax.set_title(time[time_index])\n",
    "\n",
    "    plot_name = savefile_prefix + '_time_' + str(time_index).zfill(3) + '.png'\n",
    "    plt.savefig(plot_name)\n",
    "    file_names.append(plot_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    " \n",
    "# Create the frames\n",
    "frames = []\n",
    "imgs = glob.glob(\"*.png\")\n",
    "imgs.sort() \n",
    "for i in imgs:\n",
    "    new_frame = Image.open(i)\n",
    "    frames.append(new_frame)\n",
    "\n",
    "# Save into loop\n",
    "frames[0].save('Oil_Scenario_' + scenario[file_id]+ '.gif', format='GIF',\n",
    "               append_images=frames[1:],\n",
    "               save_all=True,\n",
    "               duration=300, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_str = ' '.join(file_names)\n",
    "os.system('rm ' + file_name_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Oil_Scenario_' + scenario[file_id]+ '.gif') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
